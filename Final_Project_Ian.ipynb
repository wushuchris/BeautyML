{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d407d583-e4ac-46ed-b7a1-abedbb2a3a0b",
   "metadata": {},
   "source": [
    "# LIST OF OTHER DATASETS WE PROBABLY SHOULD CONSIDER USING INSTEAD OR SUPPLEMENTING WITH\n",
    "\n",
    "https://www.chicagofaces.org/\n",
    "https://github.com/HCIILAB/SCUT-FBP5500-Database-Release\n",
    "https://github.com/clementapa/CelebFaces_Attributes_Classification\n",
    "https://wilmabainbridge.com/facememorability2.html\n",
    "https://www.kaggle.com/datasets/jessicali9530/lfw-dataset\n",
    "https://vis-www.cs.umass.edu/lfw/#:~:text=The%20data%20set%20contains%20more,the%20Viola%2DJones%20face%20detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3498554-9d2a-4dee-9511-d8ea1a439655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp312-cp312-macosx_10_9_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-macosx_11_0_arm64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, termcolor, tensorboard-data-server, optree, ml-dtypes, grpcio, google-pasta, gast, astunparse, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "Successfully installed astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 keras-3.6.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 optree-0.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment these lines if you need to install the packages\n",
    "# !pip install numpy\n",
    "# !pip install opencv-python-headless  # cv2 for headless environments\n",
    "# !pip install mediapipe\n",
    "# !pip install tensorflow\n",
    "# !pip install dlib\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install tqdm\n",
    "# !pip install requests\n",
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37609374-4b67-4af2-b8a5-c70d956007ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All essential libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# THIS TAKES ABOUT 10-15 SECONDS TO RUN\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Image processing and visualization\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Import Seaborn for heatmap\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Deep Learning (TensorFlow)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Utility for progress bars if we decide to use them\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utility for downloading files from the internet\n",
    "import requests  \n",
    "\n",
    "# Confirmation message\n",
    "print(\"All essential libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec59b18-da24-45f5-a348-9f054c23253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download...\n",
      "Download complete!\n",
      "Extraction complete!\n",
      "Cleanup complete. File is ready for use!\n"
     ]
    }
   ],
   "source": [
    "# This cell downloads and extracts the 'shape_predictor_68_face_landmarks.dat' file, which is a pre-trained model\n",
    "# for detecting 68 specific facial landmarks. This file is required for facial feature detection tasks, such as\n",
    "# identifying key facial points for analysis.\n",
    "# \n",
    "# After downloading, the file is extracted from the .bz2 compressed format and saved as 'shape_predictor_68_face_landmarks.dat'.\n",
    "# This file will be used later in the notebook when we perform landmark-based facial feature extraction.\n",
    "# \n",
    "# For more details about the purpose and function of this file, please refer to the appendix.\n",
    "\n",
    "# URL for the shape predictor file\n",
    "url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "output_path = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "\n",
    "# Download the .bz2 file\n",
    "print(\"Starting download...\")\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Failed to download. Status code:\", response.status_code)\n",
    "\n",
    "# Extract the .bz2 file\n",
    "import bz2\n",
    "import shutil\n",
    "\n",
    "with bz2.BZ2File(output_path, 'rb') as file, open(\"shape_predictor_68_face_landmarks.dat\", 'wb') as f_out:\n",
    "    shutil.copyfileobj(file, f_out)\n",
    "print(\"Extraction complete!\")\n",
    "\n",
    "# Optional: delete the .bz2 file after extraction\n",
    "import os\n",
    "os.remove(output_path)\n",
    "print(\"Cleanup complete. File is ready for use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ee4577-3d52-46c8-b1d6-e6b6d6d91b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Axes.boxplot() got an unexpected keyword argument 'tick_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Boxplot\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mboxplot([female_ratings, male_ratings], vert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, patch_artist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                  tick_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Updated parameter name\u001b[39;00m\n\u001b[1;32m     41\u001b[0m                  boxprops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgreen\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     42\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoxplot of Mean Ratings by Gender\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Rating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "\u001b[0;31mTypeError\u001b[0m: Axes.boxplot() got an unexpected keyword argument 'tick_labels'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HISTOGRAMS AND BOX PLOTS OF MALE AND FEMALE RATINGS\n",
    "\n",
    "# Load the ratings data\n",
    "ratings_df = pd.read_csv('london_faces_ratings.csv')\n",
    "\n",
    "# Load the info data to get gender information\n",
    "info_df = pd.read_csv('london_faces_info.csv')\n",
    "\n",
    "# Calculate the mean rating for each individual (ignoring the initial columns and focusing on ratings)\n",
    "mean_ratings = ratings_df.iloc[:, 3:].mean(axis=0)\n",
    "mean_ratings.index = mean_ratings.index.str.replace('X', '')  # Strip 'X' prefix for matching filenames\n",
    "\n",
    "# Convert mean ratings to a DataFrame\n",
    "mean_ratings_df = pd.DataFrame(mean_ratings, columns=['mean_rating']).reset_index()\n",
    "mean_ratings_df.rename(columns={'index': 'face_id'}, inplace=True)\n",
    "\n",
    "# Ensure face_id is of type int for the merge\n",
    "mean_ratings_df['face_id'] = mean_ratings_df['face_id'].astype(int)\n",
    "\n",
    "# Merge with info_df to get gender information\n",
    "merged_df = pd.merge(mean_ratings_df, info_df, on='face_id', how='left')\n",
    "\n",
    "# Separate the mean ratings by gender\n",
    "female_ratings = merged_df[merged_df['face_gender'] == 'female']['mean_rating']\n",
    "male_ratings = merged_df[merged_df['face_gender'] == 'male']['mean_rating']\n",
    "\n",
    "# Plotting the distribution of mean ratings as a histogram and boxplot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(female_ratings, bins=20, edgecolor='black', color='pink', alpha=0.7, label='Female Ratings')\n",
    "axes[0].hist(male_ratings, bins=20, edgecolor='black', color='blue', alpha=0.7, label='Male Ratings')\n",
    "axes[0].set_title(\"Histogram of Mean Ratings by Gender\")\n",
    "axes[0].set_xlabel(\"Mean Rating\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot([female_ratings, male_ratings], vert=False, patch_artist=True,\n",
    "                 tick_labels=['Female', 'Male'],  # Updated parameter name\n",
    "                 boxprops=dict(facecolor='lightgreen'))\n",
    "axes[1].set_title(\"Boxplot of Mean Ratings by Gender\")\n",
    "axes[1].set_xlabel(\"Mean Rating\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6514d3d-746a-46f1-b6e8-997507856ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE RATING AND SUBJECT FILES AND FILTER BY SEX (NO PHOTOS MANIPULATED HERE)\n",
    "# THE FIRST TWO OUTPUTTED TABLES ARE EXCERPTS FROM THE RAW DATASETS\n",
    "# THE THIRD TABLE IS THE FULL MERGED FILE; THE FOURTH IS FILTERED FOR JUST FEMALES\n",
    "# FACE_ID REFERENCES THE FIRST SUBJECT, FOR WHICH THERE ARE ABOUT 2500 ROWS OF INDIVIDUAL RATINGS\n",
    "\n",
    "# Load the datasets\n",
    "info_df = pd.read_csv('london_faces_info.csv')\n",
    "ratings_df = pd.read_csv('london_faces_ratings.csv')\n",
    "\n",
    "# Display the first few rows of each DataFrame to understand their structure\n",
    "print(\"Info DataFrame from london_faces_info.csv:\")\n",
    "display(info_df.head())\n",
    "\n",
    "print(\"\\nRatings DataFrame from london_faces_ratings.csv:\")\n",
    "display(ratings_df.head())\n",
    "\n",
    "# Reshape ratings_df to have one row per rater and one column for the face_id\n",
    "ratings_melted = ratings_df.melt(id_vars=['rater_sex', 'rater_sexpref', 'rater_age'], var_name='face_id', value_name='attractiveness_score')\n",
    "\n",
    "# Clean up the face_id column to remove 'X' and ensure it's string formatted\n",
    "ratings_melted['face_id'] = ratings_melted['face_id'].str.replace('X', '')\n",
    "\n",
    "# Convert face_id to integer to match the info_df type\n",
    "ratings_melted['face_id'] = ratings_melted['face_id'].astype(int)\n",
    "\n",
    "# Merge the datasets on 'face_id'\n",
    "merged_df = pd.merge(ratings_melted, info_df, on='face_id', how='left')\n",
    "\n",
    "# Display the merged DataFrame to confirm the merge\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "display(merged_df.head())\n",
    "\n",
    "# Filter for females or males based on the column 'face_gender'\n",
    "filtered_gender = 'female'  # Set this to 'female' or 'male' as needed\n",
    "filtered_df = merged_df[merged_df['face_gender'] == filtered_gender]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(f\"\\nFiltered DataFrame for {filtered_gender}s:\")\n",
    "display(filtered_df.head())\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file for review\n",
    "filtered_output_file = f'merged_faces_data_{filtered_gender}.csv'\n",
    "filtered_df.to_csv(filtered_output_file, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6be41-c3c3-4e1c-b5f3-638da7db92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW TOP-BOTTOM 3 FACES BY ATTRACTIVENESS (USES FILTERED SUBJECT DATAFRAME)\n",
    "\n",
    "# Calculate the mean attractiveness rating for each individual\n",
    "mean_ratings = filtered_df.groupby('face_id')['attractiveness_score'].mean()\n",
    "\n",
    "# Reset the index to have face_id as a column\n",
    "mean_ratings = mean_ratings.reset_index()\n",
    "\n",
    "# Find the top 3 and bottom 3 rated individuals overall\n",
    "top_rated = mean_ratings.nlargest(3, 'attractiveness_score')\n",
    "bottom_rated = mean_ratings.nsmallest(3, 'attractiveness_score')\n",
    "\n",
    "# Function to load and preprocess an image by filename\n",
    "def load_image(face_id):\n",
    "    face_id_str = str(face_id).zfill(3)  # Ensure face_id has leading zeros\n",
    "    image_path = os.path.join('neutral_front', face_id_str + '_03.jpg')\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is not None:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for display\n",
    "    return None\n",
    "\n",
    "# Display the images in a 2x6 grid (2 rows for top and bottom)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# Top rated images\n",
    "for i, image_id in enumerate(top_rated['face_id']):\n",
    "    image = load_image(image_id)\n",
    "    if image is not None:\n",
    "        axes[0, i].imshow(image)\n",
    "        axes[0, i].set_title(f\"Top {i+1} Rated: ID {image_id}\\nRating: {top_rated['attractiveness_score'].iloc[i]:.2f}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Bottom rated images\n",
    "for i, image_id in enumerate(bottom_rated['face_id']):\n",
    "    image = load_image(image_id)\n",
    "    if image is not None:\n",
    "        axes[1, i].imshow(image)\n",
    "        axes[1, i].set_title(f\"Bottom {i+1} Rated: ID {image_id}\\nRating: {bottom_rated['attractiveness_score'].iloc[i]:.2f}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37806e4-ba87-41d8-bb19-c81bb0ed8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAYS THE STANDARDIZED IMAGES OF THE 49 FEMALE SUBJECTS\n",
    "# THESE IMAGES ARE SPECIFICALLY FROM THE NEUTRAL_FRONT SUBFOLDER IN THE ACTIVE DIRECTORY\n",
    "# THE ID CORRESPONDS TO THE FACE_ID AND FIRST 3 DIGITS OF THE JPG FILE NAMES\n",
    "\n",
    "# Extract the relevant image IDs and zero-fill to match filename format\n",
    "filtered_image_ids = set(filtered_df['face_id'].astype(str).str.zfill(3))\n",
    "\n",
    "# Define a function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Unable to read image at {image_path}. Skipping.\")\n",
    "            return None  # Return None if the image can't be read\n",
    "        image = cv2.resize(image, (1000, 1000))  # Resize image for consistent resolution\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        return gray_image\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# List to store processed images and their paths\n",
    "processed_images = []\n",
    "\n",
    "# Process each image in the 'neutral_front' folder matching filtered image IDs\n",
    "for subdir, _, files in os.walk('.'):  # Start from the current directory\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            image_id = file.split('_')[0]\n",
    "            # Check if the image is in 'neutral_front' and matches a filtered ID\n",
    "            if 'neutral_front' in subdir and image_id in filtered_image_ids:\n",
    "                image_path = os.path.join(subdir, file)\n",
    "                gray_image = preprocess_image(image_path)\n",
    "                if gray_image is not None:\n",
    "                    processed_images.append((image_id, image_path, gray_image))  # Store ID, path, and image\n",
    "\n",
    "# Sort processed images by image ID in ascending order\n",
    "processed_images.sort(key=lambda x: int(x[0]))\n",
    "\n",
    "# Display up to 50 processed images in a grid\n",
    "max_images = 50\n",
    "num_images = min(len(processed_images), max_images)\n",
    "fig, axes = plt.subplots(num_images // 5 + (num_images % 5 > 0), 5, figsize=(15, num_images * 3 // 5))\n",
    "axes = axes.flatten() if num_images > 1 else [axes]\n",
    "\n",
    "for i in range(num_images):\n",
    "    image_id, image_path, gray_image = processed_images[i]\n",
    "    axes[i].imshow(gray_image, cmap='gray')\n",
    "    axes[i].set_title(f\"ID: {image_id}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for j in range(num_images, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ae7a9-95fd-4a02-81e5-ddca007ae5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADS THE DLIB FACE DETECTOR PACKAGE AND FACIAL LANDMARK DETECTOR\n",
    "# DISPLAYS X,Y PIXEL COORDINATES FOR THE 68 DEFINED FACIAL LANDMARKS\n",
    "# OVERLAYS THE LANDMARKS WITH THE DLIB LANDMARK NUMBERING ON A FACE FOR ILLUSTRATION PURPOSES\n",
    "\n",
    "# Load dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Load the filtered DataFrame containing the IDs of female faces\n",
    "filtered_gender = 'female'\n",
    "filtered_df = merged_df[merged_df['face_gender'] == filtered_gender]\n",
    "filtered_image_ids = set(filtered_df['face_id'].astype(str).str.zfill(3))\n",
    "\n",
    "# Define a function to extract facial landmarks from a grayscale image\n",
    "def get_landmarks(gray_image):\n",
    "    faces = detector(gray_image)\n",
    "    if len(faces) > 0:\n",
    "        landmarks = predictor(gray_image, faces[0])  # Use the first detected face\n",
    "        coords = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        return coords\n",
    "    return None\n",
    "\n",
    "# Updated function to display the image with landmarks overlaid and numbered, and coordinates printed above\n",
    "def display_landmarks(image, landmarks, image_path, scale=1.0):\n",
    "    # Format and print coordinates with landmark numbers\n",
    "    formatted_landmarks = [f\"{i}:({x}, {y})\" for i, (x, y) in enumerate(landmarks)]\n",
    "    print(f\"Facial landmarks extracted from: {image_path}\")\n",
    "    print(\"Facial Landmarks:\", formatted_landmarks)\n",
    "    print(\"\\n\")  # Add extra spacing for readability\n",
    "\n",
    "    # Set a larger figure size by scale for high readability\n",
    "    plt.figure(figsize=(10 * scale, 10 * scale))\n",
    "    plt.imshow(image, cmap='gray')  # Display the image in grayscale\n",
    "\n",
    "    # Overlay landmarks with numbering for each point\n",
    "    for i, (x, y) in enumerate(landmarks):\n",
    "        plt.plot(x, y, 'ro', markersize=8 * scale)  # Slightly larger dot size\n",
    "        plt.text(x, y, str(i), color='white', fontsize=12 * scale, ha='center', va='center', backgroundcolor='red')\n",
    "\n",
    "    # Add a title with the image path\n",
    "    plt.title(f\"Landmarks for {os.path.basename(image_path)}\", fontsize=15 * scale)\n",
    "    plt.axis('off')  # Turn off axis for a cleaner look\n",
    "    plt.show()\n",
    "\n",
    "# Sort the files by ID to ensure the lowest numbered file is processed first\n",
    "image_folder = './neutral_front'  # Set your folder path here\n",
    "filtered_files = sorted([file for file in os.listdir(image_folder) if file.endswith('.jpg') and file.split('_')[0] in filtered_image_ids])\n",
    "\n",
    "# Process images in sorted order and display the first with landmarks\n",
    "for file in filtered_files:\n",
    "    image_id = file.split('_')[0]  # Extract the ID part from the filename\n",
    "    image_path = os.path.join(image_folder, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Extract landmarks\n",
    "    landmarks = get_landmarks(gray_image)\n",
    "\n",
    "    # Display the first image with landmarks, scaled as requested\n",
    "    if landmarks:\n",
    "        display_landmarks(gray_image, landmarks, image_path, scale=2)  # Adjust scale as needed\n",
    "        break  # Stop after displaying the first image with landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b57d74-0598-4396-9115-981235fe8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNS PLAIN ENGLISH LABELS TO KEY LANDMARKS\n",
    "# MEASURES EUCLIDEAN DISTANCES BETWEEN THE X,Y COORDINATES OF THE KEY LANDMARKS\n",
    "# COMPUTES FACIAL PROPORTIONS AND RATIOS USED IN OUR MODEL\n",
    "# PUTS ALL OF THESE MEASURES INTO DATAFRAMES FOR LATER USE\n",
    "\n",
    "# Define a function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Function to compute comprehensive facial features and distances based on corrected 68 facial landmarks\n",
    "def calculate_all_features(landmarks):\n",
    "    if landmarks is None:\n",
    "        return None\n",
    "\n",
    "    # Key landmarks with corrected indices\n",
    "    key_landmarks = {\n",
    "        'left_temple': np.array(landmarks[0]),\n",
    "        'right_temple': np.array(landmarks[16]),\n",
    "        'chin_tip': np.array(landmarks[8]),\n",
    "        'nose_tip': np.array(landmarks[30]),\n",
    "        'nose_top': np.array(landmarks[27]),\n",
    "        'nose_bottom': np.array(landmarks[33]),\n",
    "        'nose_left': np.array(landmarks[31]),\n",
    "        'nose_right': np.array(landmarks[35]),\n",
    "        'left_eye_outer': np.array(landmarks[36]),\n",
    "        'left_eye_inner': np.array(landmarks[39]),\n",
    "        'right_eye_outer': np.array(landmarks[45]),\n",
    "        'right_eye_inner': np.array(landmarks[42]),\n",
    "        'mouth_left_corner': np.array(landmarks[48]),\n",
    "        'mouth_right_corner': np.array(landmarks[54]),\n",
    "        'upper_lip_center': np.array(landmarks[51]),\n",
    "        'lower_lip_center': np.array(landmarks[57]),\n",
    "        'glabella': np.array(landmarks[27]),\n",
    "    }\n",
    "\n",
    "    # Basic distances\n",
    "    basic_distances = {\n",
    "        'face_length': euclidean_distance(key_landmarks['nose_top'], key_landmarks['chin_tip']),\n",
    "        'face_width': euclidean_distance(key_landmarks['left_temple'], key_landmarks['right_temple']),\n",
    "        'eye_distance_inner': euclidean_distance(key_landmarks['left_eye_inner'], key_landmarks['right_eye_inner']),\n",
    "        'eye_distance_outer': euclidean_distance(key_landmarks['left_eye_outer'], key_landmarks['right_eye_outer']),\n",
    "        'mouth_width': euclidean_distance(key_landmarks['mouth_left_corner'], key_landmarks['mouth_right_corner']),\n",
    "        'nose_width': euclidean_distance(key_landmarks['nose_left'], key_landmarks['nose_right']),\n",
    "        'nose_length': euclidean_distance(key_landmarks['nose_top'], key_landmarks['nose_bottom']),\n",
    "        'lip_height': euclidean_distance(key_landmarks['upper_lip_center'], key_landmarks['lower_lip_center']),\n",
    "        'left_eye_width': euclidean_distance(key_landmarks['left_eye_outer'], key_landmarks['left_eye_inner']),\n",
    "        'interocular_distance': euclidean_distance(key_landmarks['left_eye_outer'], key_landmarks['right_eye_outer']),\n",
    "    }\n",
    "\n",
    "    # Symmetry-related distances\n",
    "    symmetry_distances = {\n",
    "        'left_eye_to_nose_tip': euclidean_distance(key_landmarks['left_eye_inner'], key_landmarks['nose_tip']),\n",
    "        'right_eye_to_nose_tip': euclidean_distance(key_landmarks['right_eye_inner'], key_landmarks['nose_tip']),\n",
    "        'left_mouth_to_nose_tip': euclidean_distance(key_landmarks['mouth_left_corner'], key_landmarks['nose_tip']),\n",
    "        'right_mouth_to_nose_tip': euclidean_distance(key_landmarks['mouth_right_corner'], key_landmarks['nose_tip']),\n",
    "        'left_nose_to_nose_tip': euclidean_distance(key_landmarks['nose_left'], key_landmarks['nose_tip']),\n",
    "        'right_nose_to_nose_tip': euclidean_distance(key_landmarks['nose_right'], key_landmarks['nose_tip']),\n",
    "        'left_eye_to_glabella': euclidean_distance(key_landmarks['left_eye_outer'], key_landmarks['glabella']),\n",
    "        'right_eye_to_glabella': euclidean_distance(key_landmarks['right_eye_outer'], key_landmarks['glabella']),\n",
    "    }\n",
    "\n",
    "    # Feature ratios and proportions used in analysis\n",
    "    features = {\n",
    "        'golden_ratio': basic_distances['face_length'] / basic_distances['face_width'] if basic_distances['face_width'] != 0 else np.nan,\n",
    "        'eye_to_mouth_ratio': basic_distances['eye_distance_inner'] / basic_distances['mouth_width'] if basic_distances['mouth_width'] != 0 else np.nan,\n",
    "        'eye_spacing_ratio': basic_distances['eye_distance_outer'] / basic_distances['face_width'] if basic_distances['face_width'] != 0 else np.nan,\n",
    "        'nose_to_face_length_ratio': basic_distances['nose_length'] / basic_distances['face_length'] if basic_distances['face_length'] != 0 else np.nan,\n",
    "        'mouth_to_nose_ratio': basic_distances['mouth_width'] / basic_distances['nose_width'] if basic_distances['nose_width'] != 0 else np.nan,\n",
    "        'lip_height_to_mouth_ratio': basic_distances['lip_height'] / basic_distances['mouth_width'] if basic_distances['mouth_width'] != 0 else np.nan,\n",
    "        \n",
    "        # Symmetry ratios using precomputed distances\n",
    "        'eye_symmetry_ratio': abs(symmetry_distances['left_eye_to_nose_tip'] / symmetry_distances['right_eye_to_nose_tip'] - 1) if symmetry_distances['right_eye_to_nose_tip'] != 0 else np.nan,\n",
    "        'mouth_symmetry_ratio': abs(symmetry_distances['left_mouth_to_nose_tip'] / symmetry_distances['right_mouth_to_nose_tip'] - 1) if symmetry_distances['right_mouth_to_nose_tip'] != 0 else np.nan,\n",
    "        'nose_symmetry_ratio': abs(symmetry_distances['left_nose_to_nose_tip'] / symmetry_distances['right_nose_to_nose_tip'] - 1) if symmetry_distances['right_nose_to_nose_tip'] != 0 else np.nan,\n",
    "        \n",
    "        'eye_positioning_ratio': basic_distances['left_eye_width'] / basic_distances['eye_distance_inner'] if basic_distances['eye_distance_inner'] != 0 else np.nan,\n",
    "        'nose_dimensions_ratio': basic_distances['nose_length'] / basic_distances['nose_width'] if basic_distances['nose_width'] != 0 else np.nan,\n",
    "        'mouth_to_face_width_ratio': basic_distances['mouth_width'] / basic_distances['face_width'] if basic_distances['face_width'] != 0 else np.nan,\n",
    "        'eye_mouth_interaction': (basic_distances['eye_distance_outer'] / basic_distances['face_length']) * (basic_distances['mouth_width'] / basic_distances['face_width']) if basic_distances['face_length'] != 0 and basic_distances['face_width'] != 0 else np.nan,        \n",
    "    }\n",
    "\n",
    "    return features, basic_distances, symmetry_distances, key_landmarks\n",
    "\n",
    "# Modified process_features function\n",
    "def process_features(processed_images, filtered_df, num_subjects=None):\n",
    "    if not isinstance(processed_images, list):\n",
    "        raise ValueError(\"processed_images must be a list.\")\n",
    "    \n",
    "    if num_subjects is not None and (not isinstance(num_subjects, int) or num_subjects < 0):\n",
    "        raise ValueError(\"num_subjects must be a non-negative integer.\")\n",
    "\n",
    "    if 'face_id' not in filtered_df.columns:\n",
    "        print(\"Error: 'face_id' column is missing in the filtered DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    filtered_df = filtered_df.copy()\n",
    "    filtered_df['face_id'] = filtered_df['face_id'].astype(str).str.zfill(3)\n",
    "    valid_face_ids = set(filtered_df['face_id'])\n",
    "\n",
    "    features_list = []\n",
    "\n",
    "    for index, item in enumerate(processed_images):\n",
    "        if num_subjects is not None and index >= num_subjects:\n",
    "            break\n",
    "\n",
    "        if not isinstance(item, tuple) or len(item) != 3:\n",
    "            print(f\"Warning: Skipping an item in processed_images that does not have exactly three elements: {item}\")\n",
    "            continue\n",
    "\n",
    "        _, image_path, gray_image = item\n",
    "        face_id = os.path.basename(image_path).split('_')[0].zfill(3)\n",
    "        \n",
    "        if 'neutral_front' not in image_path or face_id not in valid_face_ids:\n",
    "            continue\n",
    "\n",
    "        landmarks = get_landmarks(gray_image)\n",
    "        if landmarks is None:\n",
    "            print(f\"No landmarks found for image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate features, distances, and landmarks\n",
    "        features, basic_distances, symmetry_distances, key_landmarks = calculate_all_features(landmarks)\n",
    "\n",
    "        # Add metadata and calculated values to the features dictionary\n",
    "        features['face_id'] = face_id\n",
    "        features['image_path'] = image_path\n",
    "        \n",
    "        # Combine all results into one dictionary\n",
    "        full_data = {**features, **basic_distances, **symmetry_distances, **key_landmarks}\n",
    "\n",
    "        features_list.append(full_data)\n",
    "\n",
    "    all_features_df = pd.DataFrame(features_list)\n",
    "\n",
    "    if not all_features_df.empty:\n",
    "        display(all_features_df.head())\n",
    "    else:\n",
    "        print(\"No features extracted from the provided images.\")\n",
    "\n",
    "    return all_features_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'filtered_df' is the DataFrame that contains only the selected 'face_id's for the required gender\n",
    "all_features_df = process_features(processed_images, filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecdf59-b6a4-4f8c-af42-258cabe40a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR AUDITING PURPOSES, THIS SHOWS WHAT GOES INTO A PARTICULAR FEATURE\n",
    "# THIS IS NOT FULLY FLESHED OUT AND VALIDATED\n",
    "# THE INTENTION IS TO GENERATE SOMETHING LIKE THIS, SOMEWHAT MORE DYNAMICALLY, FOR ALL THE FEATURES\n",
    "# SOME OTHER FAILED ATTEMPTS AT THIS ARE DOWN AT THE BOTTOM OF THE NOTEBOOK DESIGNATED AS MARKDOWN CELLS SO THEY DON'T RUN INADVERTENTLY\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Define a function to extract facial landmarks from a grayscale image\n",
    "def get_landmarks(gray_image):\n",
    "    faces = detector(gray_image)\n",
    "    if len(faces) > 0:\n",
    "        landmarks = predictor(gray_image, faces[0])  # Use the first detected face\n",
    "        coords = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        return coords\n",
    "    return None\n",
    "\n",
    "# Function to display relevant landmarks, names, and lines for interpreting \"lip_to_face_proportion\"\n",
    "def display_lip_to_face_proportion(image, key_landmarks, basic_distances, features, image_path):\n",
    "    plt.figure(figsize=(11, 11))  # Single plot for image\n",
    "\n",
    "    # Display the image with relevant landmarks and lines\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Only show upper lip center, lower lip center, nose top, and chin tip landmarks\n",
    "    relevant_landmarks = {\n",
    "        'nose_top': key_landmarks['nose_top'],\n",
    "        'upper_lip_center': key_landmarks['upper_lip_center'],\n",
    "        'lower_lip_center': key_landmarks['lower_lip_center'],\n",
    "        'chin_tip': key_landmarks['chin_tip']\n",
    "    }\n",
    "\n",
    "    # Plot relevant landmarks and annotate with names\n",
    "    for name, (x, y) in relevant_landmarks.items():\n",
    "        plt.plot(x, y, 'ro', markersize=6)\n",
    "        offset = 25\n",
    "        plt.text(x + offset, y, name, color='red', fontsize=10, ha='center', backgroundcolor='white')\n",
    "\n",
    "    # Draw lip height line (blue) and face length line (green)\n",
    "    lip_height_points = (relevant_landmarks['upper_lip_center'], relevant_landmarks['lower_lip_center'])\n",
    "    face_length_points = (relevant_landmarks['nose_top'], relevant_landmarks['chin_tip'])\n",
    "    plt.plot(\n",
    "        [lip_height_points[0][0], lip_height_points[1][0]],\n",
    "        [lip_height_points[0][1], lip_height_points[1][1]],\n",
    "        'b-', linewidth=1.5\n",
    "    )\n",
    "    plt.plot(\n",
    "        [face_length_points[0][0], face_length_points[1][0]],\n",
    "        [face_length_points[0][1], face_length_points[1][1]],\n",
    "        'g-', linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Annotate the \"lip_to_face_proportion\" at the top left corner\n",
    "    lip_to_face_proportion = features.get('lip_to_face_proportion', \"N/A\")\n",
    "    plt.text(\n",
    "        20, 40, f\"Lip to Face Proportion: {lip_to_face_proportion:.2f}\" if isinstance(lip_to_face_proportion, (float, int)) else \"Lip to Face Proportion: N/A\",\n",
    "        color='white', fontsize=12, backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "    # Display formulas and measurements\n",
    "    formula_text = (\n",
    "        \"Formulas:\\n\"\n",
    "        \"lip_to_face_proportion = lip_height / face_length\\n\"\n",
    "        \"lip_height = distance between [51] and [57]\\n\"\n",
    "        \"face_length = distance between [27] and [8]\"\n",
    "    )\n",
    "    plt.text(\n",
    "        20, 400, formula_text, color='white', fontsize=10, backgroundcolor='black', ha='left'\n",
    "    )\n",
    "    face_length = basic_distances.get('face_length', None)\n",
    "    lip_height = basic_distances.get('lip_height', None)\n",
    "    distance_text_y = 180\n",
    "    line_spacing = 50\n",
    "    if face_length is not None:\n",
    "        plt.text(\n",
    "            60, distance_text_y, f\"Face Length: {face_length:.2f}\",\n",
    "            color='green', fontsize=10, backgroundcolor='white'\n",
    "        )\n",
    "        distance_text_y += line_spacing\n",
    "    if lip_height is not None:\n",
    "        plt.text(\n",
    "            60, distance_text_y, f\"Lip Height: {lip_height:.2f}\",\n",
    "            color='blue', fontsize=10, backgroundcolor='white'\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Landmarks and Lip to Face Proportion for {os.path.basename(image_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load the filtered DataFrame and get valid female image IDs\n",
    "filtered_df = pd.read_csv('merged_faces_data_female.csv')  # Adjust to your actual filtered DataFrame file\n",
    "filtered_df['face_id'] = filtered_df['face_id'].astype(str).str.zfill(3)\n",
    "valid_face_ids = set(filtered_df['face_id'])\n",
    "\n",
    "# Folder containing images and sorting files by name\n",
    "image_folder = './neutral_front'\n",
    "filtered_files = sorted([file for file in os.listdir(image_folder) if file.endswith('.jpg') and file.split('_')[0] in valid_face_ids])\n",
    "\n",
    "# Iterate through each filtered image and display landmarks for the first matching file\n",
    "for file in filtered_files:\n",
    "    image_path = os.path.join(image_folder, file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {file}\")\n",
    "        continue\n",
    "    \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    landmarks = get_landmarks(gray_image)\n",
    "\n",
    "    if landmarks:\n",
    "        # Compute distances and features using pre-defined key landmarks\n",
    "        features, basic_distances, symmetry_distances, key_landmarks = calculate_all_features(landmarks)\n",
    "        display_lip_to_face_proportion(image, key_landmarks, basic_distances, features, image_path)\n",
    "        break  # Stop after displaying the first image to confirm functionality\n",
    "    else:\n",
    "        print(f\"No landmarks found for image: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e388b75-9247-4ed6-a7c0-aaae0bf6eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the ratings data for analysis\n",
    "\n",
    "# Melt the ratings_df to make each rating in a single column under a new 'image_id' column\n",
    "ratings_long_df = ratings_df.melt(\n",
    "    id_vars=['rater_sex', 'rater_sexpref', 'rater_age'],\n",
    "    var_name='image_id',\n",
    "    value_name='attractiveness_score'\n",
    ")\n",
    "\n",
    "# Standardize 'image_id' by removing 'X' and padding to match feature image IDs\n",
    "ratings_long_df['image_id'] = ratings_long_df['image_id'].str.replace('X', '').str.zfill(3)\n",
    "\n",
    "# Display a preview of the reshaped DataFrame\n",
    "display(ratings_long_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdc26a-feaa-447b-896d-dc81ec70f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE MERGES THE RATINGS DATA WITH THE FACIAL FEATURES\n",
    "# OUTPUTS A LARGE DATAFRAME AND CSV FILE (100K+ ROWS) TO THE ACTIVE DRIVE\n",
    "# TAKES 30-60 SECONDS TO RUN\n",
    "\n",
    "# Rename 'face_id' to 'image_id' in all_features_df to match ratings_long_df\n",
    "all_features_df = all_features_df.rename(columns={'face_id': 'image_id'})\n",
    "\n",
    "# Define a set of dummy landmarks for feature extraction\n",
    "dummy_landmarks = [(0, 0)] * 68  # Minimal placeholder for 68 landmarks\n",
    "\n",
    "# Use dummy landmarks to get the structure of features for dynamic feature extraction\n",
    "features, _, _, _ = calculate_all_features(dummy_landmarks)  # Retrieve the features dictionary structure\n",
    "dynamic_feature_columns = list(features.keys())  # Extract feature keys dynamically\n",
    "\n",
    "# Add 'image_id' and 'attractiveness_score' to the list for merging and preview\n",
    "selected_columns = dynamic_feature_columns + ['image_id', 'attractiveness_score']\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = all_features_df.merge(ratings_long_df, on='image_id', how='left')\n",
    "    \n",
    "# Adjust display options for a cleaner table output\n",
    "pd.set_option('display.max_columns', None)  # Display all columns (useful when needed)\n",
    "pd.set_option('display.width', 1000)  # Set an arbitrary width to prevent wrapping\n",
    "\n",
    "print(f\"\\nThis code takes 30-60 seconds to fully execute; wait for message about 'Merged data saved...'\\n\")\n",
    "\n",
    "# Display only selected columns for a clean preview\n",
    "preview_df = merged_df[selected_columns]\n",
    "display(preview_df.head(3))\n",
    "display(preview_df.tail(3))\n",
    "    \n",
    "# Save the merged DataFrame as a CSV, ensuring numeric columns are saved correctly\n",
    "merged_output_file = 'merged_facial_features_with_ratings.csv'\n",
    "merged_df.to_csv(merged_output_file, index=False, float_format='%.4f')\n",
    "print(f\"Merged data saved to {merged_output_file}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850fdf2-3b11-41f9-8ba2-f0a3f8acbc44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AVERAGES THE ATTRACTIVENESS RATINGS FOR EACH SUBJECT\n",
    "# ALSO CONDENDSES THE FEATURES BY AVERAGING THEM FOR EACH SUBJECT\n",
    "# OUTPUTS A SMALL DATAFRAME AND CSV FILE TO THE ACTIVE DRIVE\n",
    "\n",
    "# Dynamically retrieve the feature columns\n",
    "# Using the feature columns dynamically extracted from `calculate_all_features`\n",
    "feature_columns = dynamic_feature_columns + ['attractiveness_score']\n",
    "\n",
    "# Calculate the mean attractiveness score and mean values for each feature dynamically\n",
    "aggregated_df = merged_df.groupby('image_id')[feature_columns].mean().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "aggregated_df.rename(columns={'attractiveness_score': 'mean_attractiveness_score'}, inplace=True)\n",
    "\n",
    "# Display the first few rows of the aggregated DataFrame\n",
    "print(\"Raw Value DataFrame:\")\n",
    "display(aggregated_df.head(3))\n",
    "display(aggregated_df.tail(3))\n",
    "\n",
    "# Save the aggregated DataFrame to a CSV file in the current directory\n",
    "output_file = 'aggregated_attractiveness_scores.csv'\n",
    "aggregated_df.to_csv(output_file, index=False)\n",
    "print(f\"Aggregated data saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c86c7-17ba-43bf-b13d-dc6081c649ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZED VARIABLES (0 TO 1)\n",
    "# USES NORMALIZATION FROM 2.2.2 OF Human-like evaluation by facial attractiveness intelligent machine.pdf\n",
    "\n",
    "# Exclude 'mean_attractiveness_score' from normalization\n",
    "features_to_normalize = [col for col in dynamic_feature_columns if col != 'mean_attractiveness_score']\n",
    "\n",
    "# Apply Min-Max normalization to each selected feature\n",
    "for feature in features_to_normalize:\n",
    "    X_min = aggregated_df[feature].min()\n",
    "    X_max = aggregated_df[feature].max()\n",
    "    aggregated_df[feature] = (aggregated_df[feature] - X_min) / (X_max - X_min)\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "print(\"Normalized Value DataFrame:\")\n",
    "display(aggregated_df.head(3))\n",
    "display(aggregated_df.tail(3))\n",
    "\n",
    "# Save the normalized DataFrame to a CSV file in the current directory\n",
    "normalized_output_file = 'normalized_attractiveness_scores.csv'\n",
    "aggregated_df.to_csv(normalized_output_file, index=False)\n",
    "print(f\"Normalized data saved to {normalized_output_file}.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f928d421-0d63-4880-9f38-188a84911b3e",
   "metadata": {},
   "source": [
    "# STANDARDIZED VARIABLES USING Z-SCORES\n",
    "# THIS DOESN'T SEEM TO CHANGE THE CORRELATION MATRIX, SO I'M NOT SURE IT WOULD LEAD TO A DIFFERENT ANALYTICAL RESULT\n",
    "# DESIGNATING IT AS RAW TEXT SO WE DON'T RUN IT INADVERTENTLY\n",
    "\n",
    "# Exclude 'mean_attractiveness_score' from standardization\n",
    "features_to_standardize = [col for col in dynamic_feature_columns if col != 'mean_attractiveness_score']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected features, then replace them in the DataFrame\n",
    "aggregated_df[features_to_standardize] = scaler.fit_transform(aggregated_df[features_to_standardize])\n",
    "\n",
    "# Display the standardized DataFrame\n",
    "print(\"Standardized DataFrame:\")\n",
    "display(aggregated_df.head(3))\n",
    "display(aggregated_df.tail(3))\n",
    "\n",
    "# Save the standardized DataFrame to a CSV file in the current directory\n",
    "standardized_output_file = 'standardized_attractiveness_scores.csv'\n",
    "aggregated_df.to_csv(standardized_output_file, index=False)\n",
    "print(f\"Standardized data saved to {standardized_output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33401367-2fb4-496b-8c2f-ea744671e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX OF ALL THE VARIABLES\n",
    "# THE CORRELATIONS WITH ATTRACTIVENESS ARE IN THE BOTTOM ROW\n",
    "\n",
    "# Ensure mean_attractiveness_score is included in the correlation calculation\n",
    "correlation_matrix = aggregated_df.drop(columns=['image_id']).corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap with the mask and the coolwarm colormap for typical correlation shading\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, \n",
    "            fmt=\".2f\", linewidths=.5, cbar_kws={\"shrink\": .8}, vmin=-1, vmax=1)\n",
    "\n",
    "# Set title for the heatmap\n",
    "plt.title(\"Lower Triangle Correlation Matrix of Standardized Facial Features and Attractiveness\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118427be-0a8a-43b5-a253-47e0651a88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FOR PREDICTIVENESS OF THE VARIABLE SET\n",
    "# GENERATES SMALLER SET OF EXPLANATORY VARIABLES WITH RELATIVELY HIGH ASSOCIATION WITH ATTRACTIVENESS \n",
    "# AND THAT ARE NOT HIGHLY CORRELATED WITH ONE ANOTHER\n",
    "\n",
    "# Step 1: Calculate correlations and p-values with `mean_attractiveness_score`\n",
    "def calculate_correlations_with_pvalues(df, target):\n",
    "    correlations = {}\n",
    "    p_values = {}\n",
    "    for column in df.columns:\n",
    "        if column != target:\n",
    "            corr, p_value = stats.pearsonr(df[column], df[target])\n",
    "            correlations[column] = corr\n",
    "            p_values[column] = p_value\n",
    "    return pd.DataFrame({\"Correlation to Attractiveness\": correlations, \"P-value of Correlation\": p_values})\n",
    "\n",
    "# Calculate correlation and p-value with `mean_attractiveness_score`\n",
    "correlations_df = calculate_correlations_with_pvalues(aggregated_df.drop(columns=['image_id']), 'mean_attractiveness_score')\n",
    "\n",
    "# Step 2: Filter significant features based on a p-value threshold (0.30)\n",
    "significant_features = correlations_df[correlations_df['P-value of Correlation'] < 0.30].index.tolist()\n",
    "\n",
    "# Create a subset DataFrame with only significant features for further analysis\n",
    "filtered_df = aggregated_df[significant_features + ['mean_attractiveness_score']]\n",
    "\n",
    "# Define a high correlation threshold to filter out highly correlated features\n",
    "high_corr_threshold = 0.69\n",
    "to_remove = []\n",
    "\n",
    "# Step 3: Identify highly correlated features and prioritize keeping those with lower p-values\n",
    "correlation_matrix = filtered_df.corr()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "            feature_i = correlation_matrix.columns[i]\n",
    "            feature_j = correlation_matrix.columns[j]\n",
    "            # Check p-values and keep the feature with the lower p-value\n",
    "            if correlations_df.loc[feature_i, 'P-value of Correlation'] < correlations_df.loc[feature_j, 'P-value of Correlation']:\n",
    "                if feature_j not in to_remove:\n",
    "                    to_remove.append(feature_j)\n",
    "            else:\n",
    "                if feature_i not in to_remove:\n",
    "                    to_remove.append(feature_i)\n",
    "\n",
    "# Final selected features by excluding those in `to_remove`\n",
    "selected_features = [feature for feature in significant_features if feature not in to_remove]\n",
    "\n",
    "# Step 4: Update the selection status in the correlations DataFrame\n",
    "correlations_df['Selected'] = correlations_df.index.isin(selected_features)\n",
    "\n",
    "# Sort the DataFrame by P-value in ascending order\n",
    "correlations_df = correlations_df.sort_values(by=\"P-value of Correlation\")\n",
    "\n",
    "# Display the final statistics for each feature\n",
    "print(\"\\nDetailed Statistics for Each Feature Sorted by P-Value:\\n\")\n",
    "display(correlations_df)\n",
    "\n",
    "# Optionally, save the results to a CSV for documentation\n",
    "correlations_df.to_csv('feature_statistics_with_selection.csv', index=True)\n",
    "print(\"Feature statistics saved to 'feature_statistics_with_selection.csv'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d040f-919f-4834-8c93-3a6eeb40d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX OF VARIABLES BELIEVED TO BE PREDICTIVE (SELECTED IN THE PREVIOUS CODE BLOCK)\n",
    "\n",
    "# Filter the aggregated DataFrame to include only selected features\n",
    "selected_features = correlations_df[correlations_df['Selected']].index.tolist()\n",
    "selected_df = aggregated_df[selected_features]\n",
    "\n",
    "# Calculate the correlation matrix for the selected features\n",
    "correlation_matrix = selected_df.corr()\n",
    "\n",
    "# Mask only the upper triangle (excluding the diagonal)\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap with the modified mask and a cool-to-warm color palette\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    mask=mask,\n",
    "    cmap=\"coolwarm\",  # Cool-to-warm palette\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    vmin=-1, vmax=1  # Set range from -1 to 1 for correlation values\n",
    ")\n",
    "\n",
    "# Add title and display the plot\n",
    "plt.title(\"Lower Triangle Correlation Matrix of Selected Features (Including Diagonal)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ad0da-1675-4af8-8eb9-9d39e8154d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSION USING VARIABLES BELIEVED TO BE PREDICTIVE\n",
    "# REMEMBER THAT THE SQUARE ROOT OF THE R-SQUARED IS THE CORRELATION OF THE MODEL WITH ATTRACTIVNESS\n",
    "# FOR EXAMPLE, A .273 R-SQ EQUATES TO A MODERATE 0.52 CORRELATION\n",
    "\n",
    "# Prepare the data\n",
    "# Ensure 'mean_attractiveness_score' is in the DataFrame for regression\n",
    "dependent_variable = 'mean_attractiveness_score'\n",
    "X = aggregated_df[selected_features]  # Use the selected features as predictors\n",
    "y = aggregated_df[dependent_variable]  # Set mean attractiveness score as the dependent variable\n",
    "\n",
    "# Add a constant to the predictors to include an intercept in the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Perform the regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6189e-d334-4670-8a58-96fa45794e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSION THAT SPLITS THE SUBJECTS INTO A TRAINING SET AND A TEST SET\n",
    "# THE TEST SET IS 20% OF THE SUBJECTS FOR A TOTAL OF 10 SINCE THERE WERE ONLY 49 FEMALES TO START WITH\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X as the explanatory variables (all columns except 'image_id' and 'mean_attractiveness_score')\n",
    "X = merged_selected_df.drop(columns=['image_id', 'mean_attractiveness_score'])\n",
    "y = merged_selected_df['mean_attractiveness_score']\n",
    "\n",
    "# Split data into training and testing sets (75% train, 25% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Add a constant to the predictors in the training set to include an intercept in the model\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)  # Also add a constant to the test set for prediction\n",
    "\n",
    "# Perform the regression on the training data\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Optional: Evaluate the model on the test set (e.g., calculating R-squared for test set)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate R-squared and RMSE for test set\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Test R²: {r2_test}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca0f89d4-4f90-4f83-8621-d7b4e5f991b6",
   "metadata": {},
   "source": [
    "# REGRESSION USING ALL VARIABLES\n",
    "# DESIGNATED AS RAW TEXT SO WE DON'T RUN IT INADVERTENTLY\n",
    "# WE HAVE TOO FEW SUBJECTS AND TOO MANY EXPLANATORY VARIABLES TO USE THIS\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the data\n",
    "# Ensure 'mean_attractiveness_score' is in the DataFrame for regression\n",
    "dependent_variable = 'mean_attractiveness_score'\n",
    "X = aggregated_df.drop(columns=['image_id', dependent_variable])  # Use all standardized metrics as predictors\n",
    "y = aggregated_df[dependent_variable]  # Set mean attractiveness score as the dependent variable\n",
    "\n",
    "# Add a constant to the predictors to include an intercept in the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Perform the regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e52497-70d4-4c6a-85aa-f7c572812c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATES A SCATTERPLOT WITH PREDICTED ATTRACTIVENESS BASED ON REGRESSION MODEL ON THE Y-AXIS\n",
    "# AND ACTUAL ATTRACTIVENSS ON THE X-AXIS\n",
    "\n",
    "# Generate predicted values from the regression model\n",
    "predicted_values = model.predict(X)\n",
    "\n",
    "# Create a scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, predicted_values, color='skyblue', label=\"Data Points\", alpha=0.7)\n",
    "plt.xlabel(\"Actual Attractiveness Rating\")\n",
    "plt.ylabel(\"Predicted Attractiveness Rating\")\n",
    "\n",
    "# Add a line of best fit\n",
    "m, b = np.polyfit(y, predicted_values, 1)\n",
    "plt.plot(y, m * y + b, color=\"blue\", linewidth=2, label=f\"Best Fit Line: y = {m:.2f}x + {b:.2f}\")\n",
    "\n",
    "# Display R-squared on the plot\n",
    "r_squared = model.rsquared\n",
    "plt.title(f\"Predicted vs. Actual Attractiveness Ratings (R² = {r_squared:.2f})\")\n",
    "\n",
    "# Construct the regression formula based on model coefficients, using iloc to access values by position\n",
    "formula_terms = [f\"{model.params.iloc[i]:.2f} * {name}\" for i, name in enumerate(X.columns[1:], start=1)]\n",
    "formula = f\"y = {model.params.iloc[0]:.2f} + \" + \" + \".join(formula_terms)\n",
    "#plt.text(2, 4, formula, fontsize=10, color='blue', backgroundcolor='white')  # Adjust position as necessary\n",
    "\n",
    "# Add legend\n",
    "#plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d7db9-4eae-4a27-aa70-c8e66fbcd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME SCATTERPLOT, BUT WITH PHOTOS FOR REFERENCE LIKE IN FIGURE 12 OF\n",
    "# Human-like evaluation by facial attractiveness intelligent machine.pdf\n",
    "\n",
    "# Load image paths with image_ids for filtering\n",
    "image_folder = './neutral_front'\n",
    "image_paths = {file.split('_')[0].zfill(3): os.path.join(image_folder, file)\n",
    "               for file in os.listdir(image_folder) if file.endswith('.jpg')}\n",
    "\n",
    "# Generate predicted values from the regression model (assuming model and X are defined)\n",
    "predicted_values = model.predict(X)\n",
    "\n",
    "# Set up the plot size, limits, and DPI for higher resolution\n",
    "plt.figure(figsize=(12, 12), dpi=150)\n",
    "plt.xlim([min(y) - 0.5, max(y) + 0.5])\n",
    "plt.ylim([min(predicted_values) - 0.5, max(predicted_values) + 0.5])\n",
    "\n",
    "# Plot each image at its corresponding actual and predicted coordinates\n",
    "for actual, predicted, image_id in zip(y, predicted_values, aggregated_df['image_id']):\n",
    "    img_path = image_paths.get(image_id.zfill(3))\n",
    "    if img_path:\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Resize the image using high-quality resampling, and set size to 50x50 for better visibility\n",
    "        img = img.resize((50, 50), resample=Image.LANCZOS)\n",
    "\n",
    "        # Place each image at its actual vs. predicted location\n",
    "        plt.imshow(img, extent=(actual - 0.25, actual + 0.25, predicted - 0.25, predicted + 0.25))\n",
    "\n",
    "# Add the best-fit line and other details\n",
    "m, b = np.polyfit(y, predicted_values, 1)\n",
    "plt.plot(y, m * y + b, color=\"blue\", linewidth=2, label=f\"Best Fit Line: y = {m:.2f}x + {b:.2f}\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Actual Attractiveness Rating\")\n",
    "plt.ylabel(\"Predicted Attractiveness Rating\")\n",
    "r_squared = model.rsquared\n",
    "plt.title(f\"Predicted vs. Actual Attractiveness Ratings (R² = {r_squared:.2f})\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdaae0-9ae0-400f-ade7-5670e42f984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCATTERPLOT OF THE TEST SET (NEED TO REFACTOR THIS CODE BECAUSE A LOT OF IT IS DUPLICATIVE)\n",
    "\n",
    "# Retain image_id separately\n",
    "image_ids = merged_selected_df['image_id']\n",
    "\n",
    "# Define X as the explanatory variables (all columns except 'image_id' and 'mean_attractiveness_score')\n",
    "X = merged_selected_df.drop(columns=['image_id', 'mean_attractiveness_score'])\n",
    "y = merged_selected_df['mean_attractiveness_score']\n",
    "\n",
    "# Split data into training and testing sets (75% train, 25% test)\n",
    "X_train, X_test, y_train, y_test, image_id_train, image_id_test = train_test_split(\n",
    "    X, y, image_ids, test_size=0.25, random_state=42)\n",
    "\n",
    "# Add a constant to the predictors in the training set to include an intercept in the model\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)  # Also add a constant to the test set for prediction\n",
    "\n",
    "# Perform the regression on the training data\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot setup\n",
    "image_folder = './neutral_front'\n",
    "image_paths = {file.split('_')[0].zfill(3): os.path.join(image_folder, file)\n",
    "               for file in os.listdir(image_folder) if file.endswith('.jpg')}\n",
    "\n",
    "# Set up the plot size, limits, and DPI for higher resolution\n",
    "plt.figure(figsize=(12, 12), dpi=150)\n",
    "plt.xlim([min(y_test) - 0.5, max(y_test) + 0.5])\n",
    "plt.ylim([min(y_pred) - 0.5, max(y_pred) + 0.5])\n",
    "\n",
    "# Plot each image at its corresponding actual and predicted coordinates\n",
    "for actual, predicted, image_id in zip(y_test, y_pred, image_id_test):\n",
    "    img_path = image_paths.get(str(image_id).zfill(3))\n",
    "    if img_path:\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Resize the image to approximately one-third smaller (e.g., 70x70)\n",
    "        img = img.resize((70, 70), resample=Image.LANCZOS)\n",
    "\n",
    "        # Place each image at its actual vs. predicted location\n",
    "        plt.imshow(img, extent=(actual - 0.25, actual + 0.25, predicted - 0.25, predicted + 0.25), aspect='auto')\n",
    "\n",
    "# Add the best-fit line\n",
    "m, b = np.polyfit(y_test, y_pred, 1)\n",
    "plt.plot(y_test, m * y_test + b, color=\"blue\", linewidth=2, label=f\"Best Fit Line: y = {m:.2f}x + {b:.2f}\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Actual Attractiveness Rating\")\n",
    "plt.ylabel(\"Predicted Attractiveness Rating\")\n",
    "plt.title(f\"Predicted vs. Actual Attractiveness Ratings (Test Set)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5bf4af-233c-4605-b144-6a44a46ed65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATES A TABLE WITH PREDICTED ATTRACTIVENESS USING NORMALIZED FEATURES AND REGRESSION COEFFICIENTS\n",
    "# THIS IS MOSTLY FOR AUDITING PURPOSES\n",
    "# CURRENTLY USING THE FULL DATA SET\n",
    "\n",
    "# Display the full table with the constant and explanatory variables and beta coefficients\n",
    "coefficients_df = pd.DataFrame({\n",
    "    \"Variable\": [\"Constant\"] + list(X.columns[1:]),\n",
    "    \"Coefficient\": [model.params.iloc[0]] + list(model.params.iloc[1:])\n",
    "})\n",
    "display(coefficients_df)\n",
    "\n",
    "# Save the coefficient table to CSV\n",
    "coefficients_df.to_csv(\"regression_coefficients.csv\", index=False)\n",
    "\n",
    "# Create a table of actual vs. predicted values and include normalized variable values for each subject\n",
    "# Add 'image_id' to the results DataFrame for identification, ensuring leading zeros\n",
    "results_df = pd.DataFrame({\n",
    "    \"image_id\": aggregated_df['image_id'].astype(str).str.zfill(3),  # Convert to string and zero-pad to 3 digits\n",
    "    \"Actual\": y,\n",
    "    \"Predicted\": predicted_values\n",
    "})\n",
    "\n",
    "# Add each normalized predictor variable to results_df\n",
    "normalized_X = X.drop(columns=\"const\")  # Drop constant column to get only predictor variables\n",
    "for column in normalized_X.columns:\n",
    "    results_df[column] = normalized_X[column].values\n",
    "\n",
    "# Add the constant and coefficients to each row\n",
    "results_df[\"Constant\"] = model.params.iloc[0]\n",
    "for i, name in enumerate(X.columns[1:], start=1):\n",
    "    results_df[f\"Coef_{name}\"] = model.params.iloc[i]\n",
    "\n",
    "# Display the final results DataFrame without converting to numeric (preserving 'image_id' as a string)\n",
    "display(results_df)\n",
    "\n",
    "# Save the actual vs. predicted values table with normalized predictors to CSV\n",
    "results_df.to_csv(\"actual_vs_predicted_with_predictors_and_coefficients.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691f245-99a3-4eb3-b96f-7af855ec00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE SUBJECTS INTO A TRAINING SET AND A TEST SET FOR THE ML MODELS\n",
    "# THE TEST SET IS 20% OF THE SUBJECTS FOR A TOTAL OF 10 SINCE THERE WERE ONLY 49 FEMALES TO START WITH\n",
    "\n",
    "# Define X as the explanatory variables (all columns except 'image_id' and 'mean_attractiveness_score')\n",
    "X = aggregated_df.drop(columns=['image_id', 'mean_attractiveness_score'])\n",
    "\n",
    "# Define y as the target variable\n",
    "y = aggregated_df['mean_attractiveness_score']\n",
    "\n",
    "# Split data into training and testing sets (75% train, 25% test)\n",
    "# The random_state variable ensures that the same test and training sets are produced every time for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes to confirm the split\n",
    "# X_train.shape: (36, 13) means that the training set has 36 samples (rows) and 13 features (columns).\n",
    "# y_train.shape: (36,) indicates that the training set has 36 target values (one for each sample).\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Uncomment these to see what the split dataframes look like\n",
    "#display(X_train)\n",
    "#display(y_train)\n",
    "#display(X_test)\n",
    "#display(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4097555-0df8-40f1-b83b-3ab590817a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the image_id and mean_attractiveness_score columns from aggregated_df for merging\n",
    "attractiveness_score_df = aggregated_df[['image_id', 'mean_attractiveness_score']]\n",
    "\n",
    "# Merge selected_df with the attractiveness_score_df based on the image_id\n",
    "# This assumes that selected_df already contains an image_id column or that we are matching by row order\n",
    "merged_selected_df = attractiveness_score_df.merge(selected_df, left_index=True, right_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b1a37-1eac-43a5-8cbb-89da6e0c795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE SUBJECTS INTO A TRAINING SET AND A TEST SET\n",
    "# THIS IS BASED ON THE FULL SET OF FEATURES (aggregated_df) NOT THE SMALLER SET OF SELECT (merged_selected_df)\n",
    "# THE TEST SET IS 20% OF THE SUBJECTS FOR A TOTAL OF 10 SINCE THERE WERE ONLY 49 FEMALES TO START WITH\n",
    "\n",
    "# Define X as the explanatory variables (all columns except 'image_id' and 'mean_attractiveness_score')\n",
    "X = aggregated_df.drop(columns=['image_id', 'mean_attractiveness_score'])\n",
    "\n",
    "# Define y as the target variable\n",
    "y = aggregated_df['mean_attractiveness_score']\n",
    "\n",
    "# Split data into training and testing sets (75% train, 25% test)\n",
    "# The random_state variable ensures that the same test and training sets are produced every time for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes to confirm the split\n",
    "# X_train.shape: (36, 13) means that the training set has 36 samples (rows) and 13 features (columns).\n",
    "# y_train.shape: (36,) indicates that the training set has 36 target values (one for each sample).\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Uncomment these to see what the split dataframes look like\n",
    "#display(X_train)\n",
    "#display(y_train)\n",
    "#display(X_test)\n",
    "#display(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76be58f-9570-40be-8cc4-648f8416a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a K-Nearest Neighbors (KNN) regression model\n",
    "# This code initializes and trains a KNN regressor using 5 neighbors on the training data (X_train, y_train).\n",
    "# After fitting the model, it makes predictions on the test set (X_test).\n",
    "# The model is then evaluated using R² (coefficient of determination) and RMSE (root mean squared error).\n",
    "# - R² Score: Indicates how well the model explains the variance in the target variable (values closer to 1 imply better performance).\n",
    "# - RMSE: Measures the average error in predictions (lower values indicate better performance).\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize and train the KNN model with 5 neighbors\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"R² Score:\", r2)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e33d0-22c1-4fae-9e0b-0464cae4d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a Support Vector Regression (SVR) model\n",
    "# This code snippet initializes and trains an SVR model using the radial basis function (RBF) kernel.\n",
    "# The parameters specified are:\n",
    "# - kernel='rbf': Uses the RBF kernel to capture non-linear relationships.\n",
    "# - C=1.0: Controls the trade-off between achieving a low training error and a low testing error.\n",
    "# - epsilon=0.1: Defines the margin within which no penalty is given to errors.\n",
    "# After training on the training data (X_train, y_train), it predicts on the test data (X_test).\n",
    "# The model's performance is evaluated using R² and RMSE:\n",
    "# - R² Score: Closer to 1 indicates a better fit.\n",
    "# - RMSE: Lower values suggest better accuracy.\n",
    "\n",
    "# Initialize and train the SVR model with a radial basis function (RBF) kernel\n",
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "\n",
    "print(\"SVR R² Score:\", r2_svr)\n",
    "print(\"SVR RMSE:\", rmse_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36931d59-154b-4f4b-a564-d7e535b22611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for SVR with GridSearchCV\n",
    "# This code snippet performs hyperparameter tuning on a Support Vector Regressor (SVR) model using GridSearchCV.\n",
    "# We define a parameter grid (`param_grid`) to explore different combinations of SVR parameters:\n",
    "# - 'C': Regularization parameter that controls the trade-off between fitting the training data and avoiding overfitting.\n",
    "# - 'epsilon': Specifies the margin within which errors are not penalized in the training data.\n",
    "# - 'kernel': Specifies the type of kernel to be used ('linear', 'rbf', 'poly'), with RBF capturing non-linear relationships.\n",
    "# GridSearchCV performs cross-validation on each combination of these parameters and selects the one with the highest R² score.\n",
    "# The best model found is then used to predict on the test set, and its performance is evaluated with R² and RMSE.\n",
    "# This approach helps to optimize the SVR model, improving prediction accuracy while controlling for overfitting.\n",
    "\n",
    "# Interpretation of Results:\n",
    "# After tuning, the SVR model's performance metrics are as follows:\n",
    "# - Tuned SVR R² Score: The low (and slightly negative) R² score suggests that the model does not explain \n",
    "#   much of the variance in attractiveness scores based on the given features, and performance may have \n",
    "#   even decreased compared to the untuned model.\n",
    "# - Tuned SVR RMSE: The RMSE is slightly worse than the untuned model, indicating that tuning did not \n",
    "#   improve the model's predictive accuracy and may have introduced overfitting to the training data.\n",
    "# \n",
    "# If results imply that the tuning process did not yield a better model. This may suggest that SVR is \n",
    "# not well-suited for this particular dataset, or that additional feature engineering is required. We could \n",
    "# also consider trying other models, such as Random Forest or linear regression, or further refine the feature set.\n",
    "\n",
    "# Define parameter grid for SVR\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with SVR\n",
    "grid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring='r2', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_svr_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best_svr = best_svr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned SVR model\n",
    "r2_best_svr = r2_score(y_test, y_pred_best_svr)\n",
    "rmse_best_svr = np.sqrt(mean_squared_error(y_test, y_pred_best_svr))\n",
    "\n",
    "print(\"Tuned SVR R² Score:\", r2_best_svr)\n",
    "print(\"Tuned SVR RMSE:\", rmse_best_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa3ca1-f80d-41d5-a251-781d21428ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor:\n",
    "# The Random Forest model is an ensemble technique that combines predictions from multiple decision \n",
    "# trees, aiming to improve accuracy and reduce overfitting. It’s often effective at capturing complex, \n",
    "# non-linear relationships and handles high-dimensional data well.\n",
    "\n",
    "# Purpose:\n",
    "# We’re using Random Forest to explore if it can capture the relationships between facial features \n",
    "# and attractiveness scores better than simpler models.\n",
    "\n",
    "# Results:\n",
    "# - R² Score: The negative R² score indicates that the model’s performance is worse than a simple \n",
    "#   baseline of predicting the mean score, suggesting weak predictive power of the current features.\n",
    "# - RMSE: Similar to previous models, indicating limited accuracy.\n",
    "\n",
    "# This suggests that additional feature engineering or alternative models may be needed.\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Random Forest R² Score:\", r2_rf)\n",
    "print(\"Random Forest RMSE:\", rmse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af557b-3696-408c-bba3-8b6c54a1cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train a Multi-Layer Perceptron (MLP) model:\n",
    "# This code builds a neural network with two hidden layers to predict attractiveness scores. \n",
    "# A custom R² metric is implemented to evaluate the model's performance during training. \n",
    "# The training process aims to capture non-linear relationships between facial features and attractiveness ratings. \n",
    "# Results will be assessed through loss and R² values in the training and validation outputs.\n",
    "# Interpretation: Negative R² values indicate that the model is not performing well, as it is worse than a naive prediction of the mean attractiveness score. \n",
    "# The loss values suggest the model is struggling to fit the training data, indicating potential issues such as insufficient complexity or inadequate training.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Custom R² metric function\n",
    "def r2_metric(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())\n",
    "\n",
    "# Define the MLP model with two hidden layers using the Input layer\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))  # Input layer\n",
    "model.add(Dense(5, activation='relu'))  # First hidden layer\n",
    "model.add(Dense(10, activation='relu'))  # Second hidden layer\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "# Compile the model with the custom R² metric\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[r2_metric])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fb3dc-41b0-4213-82a9-9b471a0df07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23fa1c-d079-4ac7-b3eb-7d8139913fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2522cc-cc31-437f-8363-0025fda44563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4203441b-3a89-4b8a-96f2-24cc24e40e65",
   "metadata": {},
   "source": [
    "## Appendix: Description of `shape_predictor_68_face_landmarks.dat`\n",
    "\n",
    "The `shape_predictor_68_face_landmarks.dat` file is a pre-trained model used for facial landmark detection, typically in conjunction with the **dlib** library. It enables the identification of 68 specific points on a face image, corresponding to various facial features, such as the eyes, nose, mouth, and jawline.\n",
    "\n",
    "### Landmark Points\n",
    "The 68 facial landmarks are distributed as follows:\n",
    "- **Jawline**: Points 1-17 outline the jawline.\n",
    "- **Eyebrows**: Points 18-22 correspond to the right eyebrow, and points 23-27 correspond to the left eyebrow.\n",
    "- **Nose**: Points 28-36 mark the bridge and bottom of the nose.\n",
    "- **Eyes**: Points 37-42 define the right eye, and points 43-48 define the left eye.\n",
    "- **Mouth**: Points 49-68 outline the outer and inner lips.\n",
    "\n",
    "### Usage\n",
    "With this model file, it is possible to:\n",
    "1. **Detect and Extract Facial Features**: This includes identifying specific facial features like eye-mouth distances, nose width, and lip symmetry.\n",
    "2. **Analyze Symmetry and Ratios**: Calculate symmetry and ratios (e.g., golden ratio) based on the spatial relationships between landmarks. This can be valuable for facial attractiveness studies.\n",
    "3. **Pose Estimation**: The relative positions of landmarks can also help infer the orientation of the face (e.g., facing forward, angled, etc.).\n",
    "\n",
    "### Application in This Project\n",
    "In this project, the `shape_predictor_68_face_landmarks.dat` file allows for extracting and analyzing geometric and symmetry-based features from facial images. These features will be used to investigate correlations between facial structures and attractiveness ratings.\n",
    "\n",
    "### Reference\n",
    "This model can be obtained from the [dlib-models GitHub repository](https://github.com/davisking/dlib-models).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4b559-7529-48a5-b46b-fcd7b8e994f6",
   "metadata": {},
   "source": [
    "# Leave the cells below as raw text for now...Ian is going to rework them so that they function properly for feature auditing purposes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8808d4f4-a486-48d3-b199-cd7f5238d886",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Define a function to extract facial landmarks from a grayscale image\n",
    "def get_landmarks(gray_image):\n",
    "    faces = detector(gray_image)\n",
    "    if len(faces) > 0:\n",
    "        landmarks = predictor(gray_image, faces[0])  # Use the first detected face\n",
    "        coords = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        return coords\n",
    "    return None\n",
    "\n",
    "# Function to display relevant landmarks, names, and lines for interpreting \"lip_to_face_proportion\"\n",
    "def display_lip_to_face_proportion_with_components(image, key_landmarks, basic_distances, features, image_path):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(22, 11))  # Two subplots for side-by-side images\n",
    "\n",
    "    # First subplot: Display only the landmarks related to \"lip_to_face_proportion\"\n",
    "    axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Only show upper lip center, lower lip center, nose top, and chin tip landmarks\n",
    "    relevant_landmarks = {\n",
    "        'nose_top': key_landmarks['nose_top'],\n",
    "        'upper_lip_center': key_landmarks['upper_lip_center'],\n",
    "        'lower_lip_center': key_landmarks['lower_lip_center'],\n",
    "        'chin_tip': key_landmarks['chin_tip']\n",
    "    }\n",
    "\n",
    "    # Plot relevant landmarks and annotate with names\n",
    "    for name, (x, y) in relevant_landmarks.items():\n",
    "        axs[0].plot(x, y, 'ro', markersize=6)\n",
    "        offset = 25\n",
    "        axs[0].text(x + offset, y, name, color='red', fontsize=10, ha='center', backgroundcolor='white')\n",
    "\n",
    "    # Draw lip height line (blue) and face length line (green)\n",
    "    lip_height_points = (relevant_landmarks['upper_lip_center'], relevant_landmarks['lower_lip_center'])\n",
    "    face_length_points = (relevant_landmarks['nose_top'], relevant_landmarks['chin_tip'])\n",
    "    axs[0].plot(\n",
    "        [lip_height_points[0][0], lip_height_points[1][0]],\n",
    "        [lip_height_points[0][1], lip_height_points[1][1]],\n",
    "        'b-', linewidth=1.5\n",
    "    )\n",
    "    axs[0].plot(\n",
    "        [face_length_points[0][0], face_length_points[1][0]],\n",
    "        [face_length_points[0][1], face_length_points[1][1]],\n",
    "        'g-', linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Annotate the \"lip_to_face_proportion\" at the top left corner\n",
    "    lip_to_face_proportion = features.get('lip_to_face_proportion', \"N/A\")\n",
    "    axs[0].text(\n",
    "        20, 40, f\"Lip to Face Proportion: {lip_to_face_proportion:.2f}\" if isinstance(lip_to_face_proportion, (float, int)) else \"Lip to Face Proportion: N/A\",\n",
    "        color='white', fontsize=12, backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "    # Display formulas and measurements\n",
    "    formula_text = (\n",
    "        \"Formulas:\\n\"\n",
    "        \"lip_to_face_proportion = lip_height / face_length\\n\"\n",
    "        \"lip_height = distance between [51] and [57]\\n\"\n",
    "        \"face_length = distance between [27] and [8]\"\n",
    "    )\n",
    "    axs[0].text(\n",
    "        20, 400, formula_text, color='white', fontsize=10, backgroundcolor='black', ha='left'\n",
    "    )\n",
    "    face_length = basic_distances.get('face_length', None)\n",
    "    lip_height = basic_distances.get('lip_height', None)\n",
    "    distance_text_y = 180\n",
    "    line_spacing = 50\n",
    "    if face_length is not None:\n",
    "        axs[0].text(\n",
    "            60, distance_text_y, f\"Face Length: {face_length:.2f}\",\n",
    "            color='green', fontsize=10, backgroundcolor='white'\n",
    "        )\n",
    "        distance_text_y += line_spacing\n",
    "    if lip_height is not None:\n",
    "        axs[0].text(\n",
    "            60, distance_text_y, f\"Lip Height: {lip_height:.2f}\",\n",
    "            color='blue', fontsize=10, backgroundcolor='white'\n",
    "        )\n",
    "\n",
    "    axs[0].set_title(f\"Landmarks and Lip to Face Proportion for {os.path.basename(image_path)}\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Second subplot: Display only key landmarks with dlib landmark numbers\n",
    "    axs[1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    for name, (x, y) in key_landmarks.items():\n",
    "        # Use dlib's landmark number based on the name mapping\n",
    "        landmark_number = name.split('_')[-1]\n",
    "        axs[1].plot(x, y, 'ro', markersize=6)\n",
    "        axs[1].text(x + 5, y, str(landmark_number), color='white', fontsize=10, ha='center', backgroundcolor='red')\n",
    "\n",
    "    axs[1].set_title(\"Key Landmarks with Dlib Landmark Numbers\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load the filtered DataFrame and get valid female image IDs\n",
    "filtered_df = pd.read_csv('merged_faces_data_female.csv')  # Adjust to your actual filtered DataFrame file\n",
    "filtered_df['face_id'] = filtered_df['face_id'].astype(str).str.zfill(3)\n",
    "valid_face_ids = set(filtered_df['face_id'])\n",
    "\n",
    "# Folder containing images and sorting files by name\n",
    "image_folder = './neutral_front'\n",
    "filtered_files = sorted([file for file in os.listdir(image_folder) if file.endswith('.jpg') and file.split('_')[0] in valid_face_ids])\n",
    "\n",
    "# Iterate through each filtered image and display landmarks for the first matching file\n",
    "for file in filtered_files:\n",
    "    image_path = os.path.join(image_folder, file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {file}\")\n",
    "        continue\n",
    "    \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    landmarks = get_landmarks(gray_image)\n",
    "\n",
    "    if landmarks:\n",
    "        # Compute distances and features using pre-defined key landmarks\n",
    "        features, basic_distances, symmetry_distances, key_landmarks = calculate_all_features(landmarks)\n",
    "        display_lip_to_face_proportion_with_components(image, key_landmarks, basic_distances, features, image_path)\n",
    "        break  # Stop after displaying the first image to confirm functionality\n",
    "    else:\n",
    "        print(f\"No landmarks found for image: {file}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d84da765-03a3-422b-8b54-5578b92253e9",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Define a function to extract facial landmarks from a grayscale image\n",
    "def get_landmarks(gray_image):\n",
    "    faces = detector(gray_image)\n",
    "    if len(faces) > 0:\n",
    "        landmarks = predictor(gray_image, faces[0])  # Use the first detected face\n",
    "        coords = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        return coords\n",
    "    return None\n",
    "\n",
    "# Function to display each feature with relevant landmarks and lines\n",
    "def display_feature(image, feature_name, feature_value, key_landmarks, basic_distances, formula_text, image_path):\n",
    "    plt.figure(figsize=(11, 11))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Get relevant landmarks for the current feature\n",
    "    relevant_landmarks = []\n",
    "    if feature_name == 'lip_to_face_proportion':\n",
    "        relevant_landmarks = [\n",
    "            ('nose_top', key_landmarks['nose_top']),\n",
    "            ('upper_lip_center', key_landmarks['upper_lip_center']),\n",
    "            ('lower_lip_center', key_landmarks['lower_lip_center']),\n",
    "            ('chin_tip', key_landmarks['chin_tip']),\n",
    "        ]\n",
    "        # Draw the lines for lip height and face length\n",
    "        plt.plot(\n",
    "            [key_landmarks['upper_lip_center'][0], key_landmarks['lower_lip_center'][0]],\n",
    "            [key_landmarks['upper_lip_center'][1], key_landmarks['lower_lip_center'][1]],\n",
    "            'b-', linewidth=1.5\n",
    "        )\n",
    "        plt.plot(\n",
    "            [key_landmarks['nose_top'][0], key_landmarks['chin_tip'][0]],\n",
    "            [key_landmarks['nose_top'][1], key_landmarks['chin_tip'][1]],\n",
    "            'g-', linewidth=1.5\n",
    "        )\n",
    "    elif feature_name == 'face_length_to_width_ratio':\n",
    "        relevant_landmarks = [\n",
    "            ('left_temple', key_landmarks['left_temple']),\n",
    "            ('right_temple', key_landmarks['right_temple']),\n",
    "            ('nose_top', key_landmarks['nose_top']),\n",
    "            ('chin_tip', key_landmarks['chin_tip']),\n",
    "        ]\n",
    "        # Draw the lines for face width and length\n",
    "        plt.plot(\n",
    "            [key_landmarks['left_temple'][0], key_landmarks['right_temple'][0]],\n",
    "            [key_landmarks['left_temple'][1], key_landmarks['right_temple'][1]],\n",
    "            'b-', linewidth=1.5\n",
    "        )\n",
    "        plt.plot(\n",
    "            [key_landmarks['nose_top'][0], key_landmarks['chin_tip'][0]],\n",
    "            [key_landmarks['nose_top'][1], key_landmarks['chin_tip'][1]],\n",
    "            'g-', linewidth=1.5\n",
    "        )\n",
    "\n",
    "    # Annotate relevant landmarks\n",
    "    for name, (x, y) in relevant_landmarks:\n",
    "        plt.plot(x, y, 'ro', markersize=6)\n",
    "        plt.text(x + 25, y, name, color='red', fontsize=10, ha='center', backgroundcolor='white')\n",
    "\n",
    "    # Annotate the feature value at the top left corner\n",
    "    plt.text(\n",
    "        20, 40, f\"{feature_name.replace('_', ' ').capitalize()}: {feature_value:.2f}\" if isinstance(feature_value, (float, int)) else f\"{feature_name.replace('_', ' ').capitalize()}: N/A\",\n",
    "        color='white', fontsize=12, backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "    # Display the formula for the current feature\n",
    "    plt.text(20, 400, formula_text, color='white', fontsize=10, backgroundcolor='black', ha='left')\n",
    "\n",
    "    plt.title(f\"{feature_name.replace('_', ' ').capitalize()} for {os.path.basename(image_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to iterate over all features and display them one by one\n",
    "def display_all_features(image, key_landmarks, basic_distances, features, image_path):\n",
    "    feature_formulas = {\n",
    "        'lip_to_face_proportion': (\n",
    "            \"lip_to_face_proportion = lip_height / face_length\\n\"\n",
    "            \"lip_height = distance between [51] and [57]\\n\"\n",
    "            \"face_length = distance between [27] and [8]\"\n",
    "        ),\n",
    "        'face_length_to_width_ratio': (\n",
    "            \"face_length_to_width_ratio = face_length / face_width\\n\"\n",
    "            \"face_length = distance between [27] and [8]\\n\"\n",
    "            \"face_width = distance between [0] and [16]\"\n",
    "        ),\n",
    "        # Additional feature formulas can be added here\n",
    "    }\n",
    "\n",
    "    for feature_name, feature_value in features.items():\n",
    "        if feature_name in feature_formulas:\n",
    "            formula_text = feature_formulas[feature_name]\n",
    "            display_feature(image, feature_name, feature_value, key_landmarks, basic_distances, formula_text, image_path)\n",
    "\n",
    "# Example usage\n",
    "# Load the filtered DataFrame and get valid female image IDs\n",
    "filtered_df = pd.read_csv('merged_faces_data_female.csv')  # Adjust to your actual filtered DataFrame file\n",
    "filtered_df['face_id'] = filtered_df['face_id'].astype(str).str.zfill(3)\n",
    "valid_face_ids = set(filtered_df['face_id'])\n",
    "\n",
    "# Folder containing images and sorting files by name\n",
    "image_folder = './neutral_front'\n",
    "filtered_files = sorted([file for file in os.listdir(image_folder) if file.endswith('.jpg') and file.split('_')[0] in valid_face_ids])\n",
    "\n",
    "# Iterate through each filtered image and display landmarks for the first matching file\n",
    "for file in filtered_files:\n",
    "    image_path = os.path.join(image_folder, file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {file}\")\n",
    "        continue\n",
    "    \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    landmarks = get_landmarks(gray_image)\n",
    "\n",
    "    if landmarks:\n",
    "        # Compute distances and features using pre-defined key landmarks\n",
    "        features, basic_distances, symmetry_distances, key_landmarks = calculate_all_features(landmarks)\n",
    "        display_all_features(image, key_landmarks, basic_distances, features, image_path)\n",
    "        break  # Stop after displaying the first image to confirm functionality\n",
    "    else:\n",
    "        print(f\"No landmarks found for image: {file}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d5a8ac6-7b8a-4b0a-aab7-f7899385564f",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Function to dynamically display each feature with its landmarks and distances based on pre-computed data\n",
    "def display_dynamic_feature(image, feature_name, key_landmarks, basic_distances, features):\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Display feature value at the top\n",
    "    feature_value = features[feature_name]\n",
    "    ax.text(20, 40, f\"{feature_name.replace('_', ' ').title()}: {feature_value:.2f}\", color='white', fontsize=12, backgroundcolor='black')\n",
    "\n",
    "    # Identify relevant distances for the feature\n",
    "    relevant_distances = {dist_name: basic_distances[dist_name] for dist_name in basic_distances if dist_name in feature_name}\n",
    "    \n",
    "    # Display the formula for the feature\n",
    "    formula_text = f\"{feature_name.replace('_', ' ')} = {' / '.join(relevant_distances.keys())}\"\n",
    "    ax.text(20, 400, f\"Formula:\\n{formula_text}\", color='white', fontsize=10, backgroundcolor='black', ha='left')\n",
    "\n",
    "    # Display measurement values for relevant distances\n",
    "    measures = \"\\n\".join([f\"{dist_name.replace('_', ' ').title()}: {distance:.2f}\" for dist_name, distance in relevant_distances.items()])\n",
    "    ax.text(20, 450, measures, color='white', fontsize=10, backgroundcolor='black', ha='left')\n",
    "\n",
    "    # Dynamically identify relevant landmarks to be plotted based on the relevant distances\n",
    "    relevant_landmarks = {}\n",
    "    for dist_name in relevant_distances:\n",
    "        for part in dist_name.split('_'):\n",
    "            for landmark_name, coord in key_landmarks.items():\n",
    "                if part in landmark_name:\n",
    "                    relevant_landmarks[landmark_name] = coord\n",
    "\n",
    "    # Plot each landmark and annotate it\n",
    "    for name, (x, y) in relevant_landmarks.items():\n",
    "        ax.plot(x, y, 'ro', markersize=6)\n",
    "        ax.text(x + 10, y, name.replace('_', ' ').title(), color='red', fontsize=10, backgroundcolor='white')\n",
    "\n",
    "    # Draw lines between relevant landmarks if more than one landmark is identified\n",
    "    landmark_points = list(relevant_landmarks.values())\n",
    "    for i in range(len(landmark_points) - 1):\n",
    "        ax.plot([landmark_points[i][0], landmark_points[i + 1][0]], [landmark_points[i][1], landmark_points[i + 1][1]], 'b-', linewidth=1.5)\n",
    "\n",
    "    # Set title and remove axes for clarity\n",
    "    ax.set_title(f\"{feature_name.replace('_', ' ').title()} for Image\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Define the path to the image and load it\n",
    "image_folder = './neutral_front'  # Specify your folder path here\n",
    "image_file = '001_03.jpg'  # Replace with your actual file name\n",
    "image_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "print(f\"Attempting to load image from path: {image_path}\")\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Verify image loading before processing\n",
    "if image is not None:\n",
    "    # Assuming landmarks, features, basic_distances, and key_landmarks are already computed\n",
    "    for feature_name in features.keys():\n",
    "        display_dynamic_feature(image, feature_name, key_landmarks, basic_distances, features)\n",
    "else:\n",
    "    print(\"Error: Unable to load image. Please check the file path and ensure the image exists.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "927fafce-114b-4ed4-b132-780c75fac67d",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Define a function to extract facial landmarks from a grayscale image\n",
    "def get_landmarks(gray_image):\n",
    "    faces = detector(gray_image)\n",
    "    if len(faces) > 0:\n",
    "        landmarks = predictor(gray_image, faces[0])  # Use the first detected face\n",
    "        coords = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        return coords\n",
    "    return None\n",
    "\n",
    "# Define a function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Function to compute key landmarks, basic distances, and features\n",
    "def compute_landmark_data(landmarks):\n",
    "    key_landmarks = {\n",
    "        'left_temple': landmarks[0],\n",
    "        'right_temple': landmarks[16],\n",
    "        'chin_tip': landmarks[8],\n",
    "        'nose_tip': landmarks[30],\n",
    "        'nose_top': landmarks[27],\n",
    "        'nose_bottom': landmarks[33],\n",
    "        'nose_left': landmarks[31],\n",
    "        'nose_right': landmarks[35],\n",
    "        'left_eye_outer': landmarks[36],\n",
    "        'left_eye_inner': landmarks[39],\n",
    "        'right_eye_outer': landmarks[45],\n",
    "        'right_eye_inner': landmarks[42],\n",
    "        'mouth_left_corner': landmarks[48],\n",
    "        'mouth_right_corner': landmarks[54],\n",
    "        'upper_lip_center': landmarks[51],\n",
    "        'lower_lip_center': landmarks[57],\n",
    "        'glabella': landmarks[27]\n",
    "    }\n",
    "\n",
    "    basic_distances = {\n",
    "        'lip_height': euclidean_distance(key_landmarks['upper_lip_center'], key_landmarks['lower_lip_center']),\n",
    "        'face_length': euclidean_distance(key_landmarks['nose_top'], key_landmarks['chin_tip']),\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        'lip_to_face_proportion': basic_distances['lip_height'] / basic_distances['face_length'] if basic_distances['face_length'] != 0 else None\n",
    "    }\n",
    "\n",
    "    return key_landmarks, basic_distances, features\n",
    "\n",
    "# Function to display relevant landmarks, names, and lines for interpreting \"lip to face proportion\"\n",
    "def display_lip_to_face_proportion_with_components(image, key_landmarks, basic_distances, features, image_path):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(22, 11))  # Two subplots for side-by-side images\n",
    "\n",
    "    # First subplot: main feature visualization\n",
    "    axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Only display key landmarks relevant to \"lip to face proportion\" with labels\n",
    "    for name, (x, y) in key_landmarks.items():\n",
    "        axs[0].plot(x, y, 'ro', markersize=6)  # Larger marker size\n",
    "        offset = 25  # Increased offset for label placement\n",
    "        axs[0].text(x + offset, y, name, color='red', fontsize=10, ha='center', backgroundcolor='white')\n",
    "\n",
    "    # Extract component measurements from basic_distances\n",
    "    lip_height = basic_distances.get('lip_height', None)\n",
    "    face_length = basic_distances.get('face_length', None)\n",
    "    lip_to_face_proportion = features.get('lip_to_face_proportion', \"N/A\")\n",
    "\n",
    "    # Draw lip height line (blue)\n",
    "    lip_height_points = (key_landmarks['upper_lip_center'], key_landmarks['lower_lip_center'])\n",
    "    axs[0].plot(\n",
    "        [lip_height_points[0][0], lip_height_points[1][0]],\n",
    "        [lip_height_points[0][1], lip_height_points[1][1]],\n",
    "        'b-', linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Draw face length line (green)\n",
    "    face_length_points = (key_landmarks['nose_top'], key_landmarks['chin_tip'])\n",
    "    axs[0].plot(\n",
    "        [face_length_points[0][0], face_length_points[1][0]],\n",
    "        [face_length_points[0][1], face_length_points[1][1]],\n",
    "        'g-', linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Annotate the \"lip to face proportion\" at the top left corner\n",
    "    axs[0].text(\n",
    "        20, 40, f\"Lip to Face Proportion: {lip_to_face_proportion:.2f}\" if isinstance(lip_to_face_proportion, (float, int)) else \"Lip to Face Proportion: N/A\",\n",
    "        color='white', fontsize=12, backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "    # Display formulas for the feature and component distances\n",
    "    formula_text = (\n",
    "        \"Formulas:\\n\"\n",
    "        \"lip_to_face_proportion = lip_height / face_length\\n\"\n",
    "        \"lip_height = distance between [51] and [57]\\n\"\n",
    "        \"face_length = distance between [27] and [8]\"\n",
    "    )\n",
    "    axs[0].text(\n",
    "        20, 400, formula_text, color='white', fontsize=10, backgroundcolor='black', ha='left'\n",
    "    )\n",
    "\n",
    "    # Place component distances under the main box with added spacing\n",
    "    distance_text_y = 180  # Starting y-position for distance annotations\n",
    "    line_spacing = 50  # Spacing between lines\n",
    "    if face_length is not None:\n",
    "        axs[0].text(\n",
    "            60, distance_text_y, f\"Face Length: {face_length:.2f}\",\n",
    "            color='green', fontsize=10, backgroundcolor='white'\n",
    "        )\n",
    "        distance_text_y += line_spacing\n",
    "    if lip_height is not None:\n",
    "        axs[0].text(\n",
    "            60, distance_text_y, f\"Lip Height: {lip_height:.2f}\",\n",
    "            color='blue', fontsize=10, backgroundcolor='white'\n",
    "        )\n",
    "\n",
    "    axs[0].set_title(f\"Landmarks and Lip to Face Proportion for {os.path.basename(image_path)}\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Second subplot: Display only the key landmarks with their names\n",
    "    axs[1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    for name, (x, y) in key_landmarks.items():\n",
    "        axs[1].plot(x, y, 'ro', markersize=6)\n",
    "        axs[1].text(x + 5, y, name, color='white', fontsize=10, ha='center', backgroundcolor='red')\n",
    "\n",
    "    axs[1].set_title(\"Key Landmarks with Labels\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load the filtered DataFrame and get valid female image IDs\n",
    "filtered_df = pd.read_csv('merged_faces_data_female.csv')  # Adjust to your actual filtered DataFrame file\n",
    "filtered_df['face_id'] = filtered_df['face_id'].astype(str).str.zfill(3)\n",
    "valid_face_ids = set(filtered_df['face_id'])\n",
    "\n",
    "# Folder containing images and sorting files by name\n",
    "image_folder = './neutral_front'\n",
    "filtered_files = sorted([file for file in os.listdir(image_folder) if file.endswith('.jpg') and file.split('_')[0] in valid_face_ids])\n",
    "\n",
    "# Iterate through each filtered image and display landmarks for the first matching file\n",
    "for file in filtered_files:\n",
    "    image_path = os.path.join(image_folder, file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {file}\")\n",
    "        continue\n",
    "    \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    landmarks = get_landmarks(gray_image)\n",
    "\n",
    "    if landmarks:\n",
    "        key_landmarks, basic_distances, features = compute_landmark_data(landmarks)\n",
    "        display_lip_to_face_proportion_with_components(image, key_landmarks, basic_distances, features, image_path)\n",
    "        break  # Stop after displaying the first image to confirm functionality\n",
    "    else:\n",
    "        print(f\"No landmarks found for image: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0ca24-a4ce-4924-92f4-4fb409ba7f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabf349-0581-492d-b40b-9ab33abea218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
